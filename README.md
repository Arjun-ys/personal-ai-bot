# üß† Personal AI Agent

A local-first RAG system that acts as a **persistent digital extension of yourself**. It doesn't just answer questions ‚Äî it organically learns your coding style, tech preferences, personality, and personal context through continuous conversation.

Built with Python, Streamlit, LangChain, and ChromaDB.

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B?logo=streamlit&logoColor=white)
![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20Store-green)
![LangChain](https://img.shields.io/badge/LangChain-Orchestration-yellow)

---

## How It Works

### Triple-Memory Architecture

The system maintains **three isolated ChromaDB collections** to separate concerns and prevent data corruption:

| Collection | Purpose | Written When |
|---|---|---|
| **Document Vault** | Factual knowledge from uploaded files (PDF, TXT, LOG) | On file upload |
| **Episodic Memory** | Timestamped conversation history | After every chat turn |
| **Core Identity** | Extracted personal facts, preferences, and communication style | Mined passively after every chat |

Every time you chat, the AI:
1. Retrieves relevant context from **all three** memory banks
2. Generates a response using your chosen LLM
3. Saves the conversation to episodic memory
4. Runs a background **fact extraction pass** that mines personal details from your message and deduplicates them before storing in the identity database

Over days and weeks, the identity collection builds into a rich profile ‚Äî your name, education, tech stack, projects, opinions, and even how you communicate.

---

### Three-Tier OCR Engine

Uploading scanned documents (like academic marksheets) triggers a cascading OCR pipeline:

| Tier | Engine | Speed | Table Support | When Used |
|---|---|---|---|---|
| 1 | **PyMuPDF** (digital text) | ‚ö° Instant | ‚ùå | Native-text PDFs |
| 2 | **Gemini Vision** | üîÑ ~2s/page | ‚úÖ Full markdown tables | Scanned/complex PDFs |
| 3 | **RapidOCR** (local) | üêå Slow | ‚ùå | Offline fallback |

**Tier 2 (Gemini Vision)** is the key innovation ‚Äî it renders each page at 300 DPI, sends it to Gemini's vision model with a strict prompt that forces:
- Markdown table output with subject-score row alignment
- Watermark/border noise rejection
- Numerical precision for scores, dates, and roll numbers

### Table-Preserving Chunker

Standard text splitters destroy table structure. This system:
- Detects markdown tables via regex before splitting
- Keeps tables as **atomic chunks** (never split mid-row)
- Splits large tables by row batches with the header repeated on each chunk
- Tags every chunk with `content_type: "text"` or `"table"` metadata

---

## Features

- **Streaming responses** ‚Äî tokens appear in real-time
- **Dual LLM support** ‚Äî Local (Ollama/Llama 3) or Cloud (Gemini Flash)
- **Memory Dashboard** ‚Äî live counts for all three collections
- **Identity Browser** ‚Äî view all extracted facts about you in the sidebar
- **Memory Management** ‚Äî per-collection clear buttons
- **Full Memory Audit** ‚Äî ask the AI to compile everything it knows about you
- **Document deduplication** ‚Äî prevents re-ingesting the same file
- **Recent conversation window** ‚Äî last 6 messages fed into prompt for coherent multi-turn dialogue

---

## Setup

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai/) installed and running (for local mode)
- Google API key (for Gemini cloud mode and Vision OCR)

### Installation

```bash
git clone https://github.com/Arjun-ys/personal-ai-bot.git
cd personal-ai-bot

python -m venv venv
# Windows
.\venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the project root:

```env
GOOGLE_API_KEY=your_google_api_key_here
```

Get your API key from [Google AI Studio](https://aistudio.google.com/apikey).

### Running

```bash
# Start Ollama (for local mode)
ollama serve
ollama pull llama3

# Launch the app
streamlit run app.py
```

---

## Project Structure

```
‚îú‚îÄ‚îÄ app.py              # Main application (all logic in one file)
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ .env                # API keys (not committed)
‚îú‚îÄ‚îÄ ai_memory_db/       # ChromaDB persistent storage (auto-created)
‚îÇ   ‚îú‚îÄ‚îÄ chroma.sqlite3
‚îÇ   ‚îî‚îÄ‚îÄ .../            # Collection data directories
‚îî‚îÄ‚îÄ README.md
```

---

## Tech Stack

| Component | Technology |
|---|---|
| UI | Streamlit |
| Vector Store | ChromaDB |
| Embeddings | HuggingFace `all-MiniLM-L6-v2` |
| LLM (Local) | Ollama + Llama 3 |
| LLM (Cloud) | Google Gemini 2.0 Flash |
| Vision OCR | Google Gemini Vision API |
| PDF Parsing | PyMuPDF |
| OCR Fallback | RapidOCR |
| Orchestration | LangChain |

---

## Usage Tips

1. **Start conversations naturally** ‚Äî tell it your name, what you're working on, what tools you use. It extracts and remembers these automatically.
2. **Upload your documents** ‚Äî resumes, marksheets, project docs, logs. They go into the vault and are never modified.
3. **Check the identity panel** ‚Äî expand "üß¨ View Known Facts About You" to see what it has learned.
4. **Run memory audits** ‚Äî click "üîé Full Memory Audit" to get a full dossier of everything it knows.
5. **Use local mode for privacy** ‚Äî Ollama keeps everything on your machine. Switch to Gemini when you need stronger reasoning.

---

## License

MIT
